# Clase 03 - An谩lisis de algoritmos

**Fecha:** 18-12-2025
**Estado:**  Completado

## Resumen en 3 l铆neas

Eficiencia medida en base a existencia de algoritmos de tiempo polin贸mico. Crecimiento asint贸tico con las notaciones $O,\Omega,\Theta$ y propiedades de estas nociones para medir tiempos de ejecuci贸n.
Propiedades sobre las funciones m谩s comunes: polinomios, logar铆tmos y exponenciales

## Preguntas Clave

1. 驴Cu谩l es la diferencia entre la notaci贸n $O$ y $\Omega$?
2. 驴Cu谩les son las ventajas y desventajas de definir la eficiencia de un algoritmo en base a si es de tiempo polinomial o no?
3. 驴Qu茅 queremos decir cuando decimos que un algor铆tmo tiene tiempo exponencial?

## Contenido

### Introducci贸n al an谩lisis de algoritmos

El an谩lisis de algoritmos implica considerar como escalan los requisitos de recursos de ellos, es decir el tiempo que demoran en ejecutarse, y el espacio que precisan para almacenar informaci贸n, a medida que crece la cantidad de datos con las que opera.

### Trazabilidad computacional

Uno de los primeros conceptos que aparecen y son fundamentales para la comprensi贸n de este tema, es el concepto de eficiencia, para la cual podemos intentar dar una primera definici贸n c贸mo:

_"Un algoritmo es eficiente si, cuando es implementado, se ejecuta r谩pidamente en instancias de entrada reales."_

Esta primera definici贸n, omite algunas ideas que son fundamentales para definir la eficiencia:

- El donde se ejecuta el algoritmo, esto es muy importante ya que algoritmos no tan bien implementados en procesadores extremadamente r谩pidos, pueden ejecutarse r谩pidamente.
- Por otro lado, 驴qu茅 es una instancia de entradas reales? Esto es muy dependiente del problema que estamos intentando resolver y las necesidades de qui茅n precisa la soluci贸n.
- Adem谩s, la definici贸n no tiene en cuenta lo que pasa con el algoritmo cuando el tama帽o de la entrada crece de forma inesperada, que es un factor que queremos considerar para determinar la eficiencia.

Veamos algunos conceptos intermedios que ser谩n importantes para acercarnos a una buena definici贸n de eficiencia.

#### Tiempos de ejecuci贸n de peores casos y b煤squeda por fuerza bruta

Lo primero que solemos aprender sobre tiempos de ejecuci贸n, es el an谩lisis del peor tiempo de ejecuci贸n; esto se hace buscando un l铆mite en el mayor tiempo de ejecuci贸n posible que el algoritmo pudiese tener sobre todas las entradas de un tama帽o $N$ dado, y ver como esto crece con $N$. Esto es lo que ya hicimos anteriormente en Programaci贸n II (a nivel muy b谩sico).
En general, se ha encontrado que el an谩lisis del peor caso de un algor铆tmo hace un buen trabajo en capturar su eficiencia en la pr谩ctica.

Por otra parte, cuando obtenemos un tiempo de ejecuci贸n, naturalmente nos surge preguntarnos: 驴que punto de referencia anal铆tico nos puede decir si un tiempo de ejecuci贸n es muy bueno o muy malo? Una primera gu铆a bastante simple es comparando con el tiempo de b煤squeda por fuerza bruta en el espacio de soluciones posibles.
Volvamos al problema de emparejamiento estable para agarrar intuici贸n. Incluso cuando el tama帽o de una instancia de entrada de emparejamiento estable es relativamente peque帽o, el espacio de b煤squeda que define es enorme, ya que hay $n!$ posibles combinaciones de parejas y nosotros necesitamos encontrar ah铆 un emparejamiento estable.
El algoritmo de fuerza bruta para este caso implicar铆a comprobar cada emparejamiento y verificar si es estable. En vez de quedarnos con esta soluci贸n, nosotros definimos un algoritmo que necesita dedicar un tiempo proporcional a $N$ para encontrar un emparejamiento estable en un campo tan grande de posibilidades. El enfoque anal铆tico y matem谩tico nos permiti贸 llegar a esta conclusi贸n sin necesidad de implementar ni ejecutar el algoritmo definido.

Esto ser谩 muy com煤n en la mayor铆a de los problemas que estudiaremos: una representaci贸n compacta, especificando un espacio de b煤squeda gigante. Para la mayor铆a de estos casos habr谩 una soluci贸n obvia de fuerza bruta.
Veremos como este enfoque no solo es demasiado lento para ser 煤til, sino tambi茅n que no proporciona absolutamente ninguna idea de la estructura del problema que estamos estudiando.

Con todo esto, consideremos la siguiente nueva definici贸n de eficiencia:

_"Un algoritmo es eficiente si se logra cualitativamente un mejor rendimiento del peor caso, en un nivel anal铆tico, que la b煤squeda por fuerza bruta."_

Pero, 驴qu茅 queremos decir con un **rendimiento cualitativamente mejor**?

#### Tiempo polinomial como definici贸n de eficiencia

Supongamos que un algoritmo cumple con la siguiente propiedad: existen constantes absolutas $c>0$ y $d>0$ tal que, para cada entrada de tama帽o $N$, su tiempo de ejecuci贸n est谩 acotado por $cN^d$ pasos computacionales primitivos. En otras palabras, su tiempo de ejecuci贸n es, como m谩ximo proporcional a $N^d$.
Luego, decimos que si esta cota de tiempo de ejecuci贸n se cumple para alg煤n $c$ y $d$, decimos que el algoritmo tiene un tiempo de ejecuci贸n polin贸mico.

Por esto, consideramos nuestra tercer definici贸n de eficiencia:

_"Un algoritmo es eficiente si tiene un tiempo de ejecuci贸n polin贸mico."_

Mientras las anteriores descripciones parec铆an demasiado vagas, 茅sta parece demasiado descriptiva. Por ejemplo, si un algoritmo tiene un tiempo de ejecuci贸n proporcional a $n^{100}$, 驴no ser铆a extremadamente ineficiente? La respuesta es que si, ser铆a extremadamente ineficiente.
Pero, por mucho que se intente motivar abstractamente la definici贸n de eficiencia en t茅rminos de tiempo polin贸mico, la realidad es m谩s simple: realmente funciona.

Los problemas para los cuales existen algoritmos de tiempo polin贸mico casi invariablemente resultan tener algoritmos con tiempos de ejecuci贸n proporcionales a polinomios que crecen de manera moderada, como $n,n\log(n)n^2,n^3$. Asimismo, problemas para los cuales no se conoce un algoritmo de tiempo polin贸mico tienden a ser muy dif铆ciles en la pr谩ctica.

Una raz贸n m谩s por la cual el formalismo matem谩tico y la evidencia emp铆rica parecen alinearse bien en el caso de la solvencia de tiempo polinomial es que la diferencia entre las tasas de crecimiento de funciones polinomiales y exponenciales es enorme. Consideremos la siguiente tabla que muestra los tiempos de ejecuci贸n para diferentes l铆mites con diferentes tama帽os de entradas; con un procesador que ejecuta un mill贸n de instrucciones de alto nivel por segundo.

![Figura 1](../img/clase3fig1.png)

Otro beneficio fundamental para esta definici贸n de eficiencia tan espec铆fica, es que se vuelve **negable**; es decir, permite expresar la no existencia de un algoritmo eficiente para un problema particular.

### rden de crecimiento asint贸tico

Como ya se discuti贸 anteriormente, por lo general estaremos contando pasos en una implementaci贸n de pseudoc贸digo de un algoritmo que se asemeja a un lenguaje de programaci贸n de alto nivel.

#### $O,\Omega$ y $\Theta$

Queremos poder expresar la tasa de crecimiento de los tiempos de ejecuci贸n y otras funciones, de una manera que sea insensible a factores constantes y t茅rminos de menor orden, es decir, nos gustar铆a tomar un tiempo de ejecuci贸n c贸mo $1.62n^2+3.5n+8$ y decir que crece c贸mo $n^2$. A continuaci贸n veremos una forma precisa de hacer esto.

##### Cotas superiores asint贸ticas

Sea $T(n)$ una funci贸n que representa el tiempo de ejecuci贸n en el peor de los casos de un algoritmo, con una entrada de tama帽o $n$. Dada otra funci贸n $f(n)$, decimos que $T(n)$ es del orden $f(n)$, o de forma equivalente, $T(n)$ es $O(f(n))$ si para un $n$ suficientemente grande, la funci贸n $T(n)$ est谩 acotada superiormente por una constante que es m煤ltiplo de $f(n)$.
Refinando un poco la definici贸n, podemos decir que $T(n)$ es $O(f(n))$ sii:

- Existen las constantes $c>0$ y $n_0\geq0$ tal que para todo $n\geq n_0$ se cumple que $T(n)\leq c\cdot f(n)$

Si esto se cumple, decimos que $T$ est谩 acotada superiormente de forma asint贸tica por $f$. Es importante notar que esta definici贸n requiere que exista una constante $c$ que funcione para todo $n$; en particular, $c$ no puede depender de $n$.

Veamos un ejemplo de c贸mo esta definici贸n nos permite expresar cotas superiores para los tiempos de ejecuci贸n:
Consideremos un algoritmo que tiene un tiempo de ejecuci贸n de la forma $T(n)=an^2+bn+c$ con $a,b,c>0$. Nos gustar铆a afirmar que cualquier funci贸n de este tipo es $O(n^2)$.
Para entender porque, notamos que para todo $n>1$, se cumple que $qn\leq qn^2$ y $r\leq rn^2$. Entonces:

- $T(n)=pn^2+qn+r\leq pn^2+qn^2+rn^2\leq(p+q+r)n^2$ para todo $n\geq 1$

Por lo que encontramos $c=p+q+r$ tal que $T$ cumple con la definici贸n de $O(-)$:

- $T(n)\leq cn^2$ para todo $n\geq n_0=1$

##### Cotas inferiores asint贸ticas

Existe una notaci贸n complementaria para las cotas inferiores. A menudo, cuando analizamos un algoritmo queremos mostrar que la cota superior que hallamos es "la mejor posible". Para esto introducimos el concepto complementario de las cotas inferiores asint贸ticas.
Lo que queremos hacer es expresar que para tama帽os de entrada arbitrariamente grandes $n$, la funci贸n $T(n)$ es por lo menos una constante m煤ltiplo de alguna funci贸n espec铆fica $f(n)$.
Con esto decimos que $T(n)$ es $\Omega(f(n))$ si existen constantes $\epsilon>0$ y $n_0\geq0$ tal que, para todo $n\geq n_0$ se cumple que $T(n)\geq\epsilon\cdot f(n)$

Esta definici贸n funciona de forma similar a $O(-)$, excepto que estamos acotando la funci贸n $T(n)$ desde abajo.

##### Cotas asint贸ticamente ajustadas

Si podemos demostrar que un tiempo de ejecuci贸n $T(n)$ es tanto $O(f(n))$ como $\Omega(f(n))$, entonces en un sentido natural, hemos obtenido la cota "correcta": $T(n)$ crece exactamente como $f(n)$ dentro de un factor constante.

Cuando esto sucede, lo denotamos diciendo que $T(n)$ es $\Theta(f(n))$, donde decimos que $f(n)$ es una cota asint贸ticamente ajustada para $T(n)$.

Podemos obtener una cota asint贸ticamente ajustada directamente calculando un l铆mite cuando $n$ tiende a infinito. Esencialmente si la raz贸n entre las funciones $f(n)$ y $g(n)$ converge a una constante positiva cuando $n$ tiende a infinito, entonces $f(n)$ es $\Theta(g(n))$.

- **(2.1) Sean dos funciones $f$ y $g$ tales que: $\lim_{n\to\infty}\frac{f(n)}{g(n)}$ existe y es un n煤mero $c>0$. Entonces $f(n)=\Theta(g(n))$**

**Demostraci贸n** 

Usaremos el hecho de que el l铆mite existe y es positivo para demostrar que $f(n)=O(g(n))$ y $f(n)=\Omega(g(n))$.

Como $\lim_{n\to\infty}\frac{f(n)}{g(n)}=c>0$, se deduce de la definici贸n de l铆mite que a partir de un cierto $n_0$ se cumple que: $\frac{c}{2}<\frac{f(n)}{g(n)}<2c$. De esto obtenemos que:

- $f(n)<2c\cdot g(n)\implies f(n)=O(g(n))$
- $f(n)>\frac{c}{2}\cdot g(n)\implies f(n)=\Omega(g(n))$

Esto concluye la prueba. $\blacksquare$

#### Propiedades de las tasas de crecimiento asint贸tico

##### Transitividad

- **(2.2):**
    1. **Si $f=O(g)$ y $g=O(h)$ entonces $f=O(h)$**
    2. **Si $f=\Omega(g)$ y $g=\Omega(h)$ entonces $f=\Omega(h)$**

**Demostraci贸n**

Probaremos solo la parte $(i)$ ya que la parte $(ii)$ es an谩loga.
Para esta parte, tenemos por hip贸tesis que:

- $f=O(g)\implies$ existen $c,n_0$ tales que a partir de $n\geq n_0$ se cumple que $f(n)\leq c\cdot g(n)$
- $g=O(h)\implies$ existen $c',n_0'$ tales que a partir de $n\geq n_0'$ se cumple que $g(n)\leq c'\cdot h(n)$

Considerando entonces $\max\{n_0,n_0'\}$, tendremos que $\forall n\geq\max\{n_0,n_0'\}$ se cumple que:

- $f(n)\leq c\cdot g(n)\leq c\cdot c'\cdot h(n)$, es decir que:
- $f(n)\leq c\cdot c'\cdot h(n)\implies f(n)=O(h(n))$

Esto es exactamente lo que buscabamos con la constante $c\cdot c'$ y el natural $\max\{n_0,n_0'\}$, lo que concluye la prueba. $\blacksquare$

De esto se deduce bastante naturalmente la transitividad para las cotas asint贸ticamente ajustadas.

- **(2.3) Si $f=\Theta(g)$ y $g=\Theta(h)$, entonces $f=\Theta(h)$**

##### Suma de funciones

Resulta 煤til tambi茅n tener resultados que relacionen la suma de dos funciones.

- **(2.4) Supongamos que $f$ y $g$ son dos funciones tales que para alguna otra funci贸n $h$, se cumple que $f=g=O(h)$. Entonces $f+g=O(h)$**

**Demostraci贸n**

Tenemos que $f=O(h)$ y $g=O(h)$, es decir que:

- $f=O(h)\implies$ existen $c,n_0$ tales que $\forall n\geq n_0: f(n)\leq c\cdot h(n)$
- $g=O(h)\implies$ existen $c',n_0'$ tales que $\forall n\geq n_0': g(n)\leq c'\cdot h(n)$

Considerando entonces $\max\{n_0,n_0'\}$, tendremos que $\forall n\geq\max\{n_0,n_0'\}$ se cumple que:

$$
\begin{aligned}
f(n)+g(n)&\leq c\cdot h(n)+c'\cdot h(n)\\
&\leq (c+c')\cdot h(n)\\
\end{aligned}
$$

Esto es exactamente lo que buscabamos con la constante $c+c'$ y el natural $\max\{n_0,n_0'\}$, lo que concluye la prueba. $\blacksquare$

Este resultado se puede generalizar para un n煤mero finito de funciones $k$, con $k\geq 2$.

- **(2.5) Sea $k\geq 2$ una constante fija, y sean $f_1,\ldots,f_k$ y $h$ funciones tales que $(\forall i=1,\ldots,k)f_i=O(h)$, entonces $f_1+\ldots+f_k=O(h)$**

Omitiremos la demostraci贸n ya que es la que vimos en 2.4 adaptada a m谩s funciones.

Existe una consecuencia de (2.4) que cubre situaciones comunes. A menudo, cuando uno analiza un algoritmo con dos partes principales, es f谩cil demostrar que una es m谩s lenta que la otra. Queremos poder afirmar que el tiempo de ejecuci贸n total del algor铆tmo es asint贸ticamente comparable con la parte m谩s lenta.

- **(2.6) Supongamos que $f$ y $g$ son dos funciones que toman valores no negativos tales que $g=O(f)$. Entonces $f+g=\Theta(f)$. En otras palabras $f$ es una cota asint贸ticamente ajustada para la funci贸n combinada $f+g$.**

**Demostraci贸n**

Claramente $f+g=\Omega(f)$, ya que para todo $n\geq n_0$:

- $f(n)+g(n)\geq f(n)$

Por lo que solo necesitar铆amos mostrar que $f+g=O(f)$, pero esto es una consecuencia directa de 2.4, pues $g=O(f)$, por lo que:

- $f+g=O(f)$

Para finalizar, juntamos ambas partes obteniendo que $f+g=\Theta(f)$. $\blacksquare$

#### L铆mites asint贸ticos para algunas funciones comunes

Hay varias funciones que aparecen frecuentemente en el an谩lisis de algoritmos, y es 煤til considerar las propiedades asint贸ticas de algunas de las m谩s b谩sicas: polinomios, logaritmos y exponenciales.

##### Polinomios

- **(2.7) Sea $f$ un polinomio de grado $d$, en el cual el coeficiente $a_d$ es positivo. Entonces $f=O(n^d)$**

**Demostraci贸n**

Escribimos $f=a_0+a_1n+a_2n^2+\ldots+a_dn^d$, donde $a_d>0$. Notemos que para todos los coeficientes $j$ con $j\leq d$ se cumple que:

- $a_jn^j\leq|a_j|n^d$

**Observaci贸n:** Notemos que el valor absoluto nace de que $a_j$ a priori puede ser negativo.

Por lo tanto cada t茅rmino en el polinomio es $O(n^d)$.
Dado que $f$ es una suma de un n煤mero constante de funciones con todas ellas de 贸rden $O(n^d)$, podemos aplicar 2.5 y concluir que:

- $f=O(n^d)$

Esto concluye la prueba. $\blacksquare$

##### Logaritmos

Recordemos que $\log_b n$ es el n煤mero $x$ tal que $b^x=n$
Una forma de obtener que tan r谩pido crece $\log_b n$ es notar que, si redondeamos hacia abajo al entero m谩s cercano, es uno menos que el n煤mero de digitos en la representaci贸n en base $b$ de $n$.
Por ejemplo $1+\log_2 n$, es el n煤mero de bits necesarios para representar $n$.

As铆 que los logaritmos son funciones que crecen muy lentamente, aunque esto ya lo sab铆amos, veamos como podemos acotar el crecimiento logar铆tmico por polinomios.

- **(2.8) Para cada base $b>1$ y cada $x>0$, tenemos que $\log_b n=O(n^x)$**

**Observaci贸n:** Por otro lado, la existencia de la siguiente propiedad:

- $\log_b n=\frac{\log_a n}{\log_a b}$

Nos permite "ignorar" la base del logar铆tmo a la hora de expresar las cotas.

##### Exponenciales

Las funciones exponenciales son de la forma $f(n)=r^n$ para alguna base constante $r$. Aqu铆 nos enfocamos en el caso en que $r>1$, que resulta en funciones que crecen muy r谩pidamente.
Podemos resumir la relaci贸n entre las funciones exponenciales y las polin贸micas por:

- **(2.9) Para cada $r>1$ y cada $d>0$, tenemos que $n^d=O(r^n)$**

Es decir, cualquier polinomio est谩 acotado superiormente por cualquier exponencial.
Solemos usar estas funciones para expresar que el tiempo de ejecuci贸n de un algoritmo crece al menos tan r谩pido como una funci贸n exponencial, por lo que podemos descartar el algoritmo sin trabajar m谩s en los detalles exactos del tiempo de ejecuci贸n. 